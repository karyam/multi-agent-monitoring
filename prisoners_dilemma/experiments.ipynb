{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prisoner's Dilemma Experiments\n",
    "\n",
    "This notebook demonstrates how to run professional Prisoner's Dilemma experiments using the Concordia framework. We'll set up the environment, configure agents, and run multiple trials with proper logging and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import Concordia components\n",
    "from concordia import language_model\n",
    "from concordia import measurements\n",
    "from concordia.associative_memory import importance_function\n",
    "from concordia.utils import measurements as measurements_lib\n",
    "from concordia.typing import scene as scene_lib\n",
    "from concordia.typing import agent as agent_lib\n",
    "\n",
    "# Import our custom environment\n",
    "from prisoners_dilemma.environment import Simulation\n",
    "from prisoners_dilemma.environment import PDPayoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Language Models and Embedder\n",
    "\n",
    "First, we'll set up the language models for both the game master and agents, along with the embedder for memory operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize language models\n",
    "model = language_model.LanguageModel()  # Replace with your preferred model\n",
    "\n",
    "# Initialize embedder (using sentence-transformers as an example)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a wrapper function for the embedder\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    return embedder.encode(text)\n",
    "\n",
    "# Initialize measurements\n",
    "measurements = measurements_lib.Measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Experiment Parameters\n",
    "\n",
    "Set up the parameters for our experiment, including the number of trials, rounds per trial, and agent configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'num_trials': 10,\n",
    "    'rounds_per_trial': 5,\n",
    "    'agent_types': [\n",
    "        'analytical',\n",
    "        'cooperative',\n",
    "        'skeptical'\n",
    "    ],\n",
    "    'random_seed': 42,\n",
    "    'output_dir': 'experiment_results'\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(EXPERIMENT_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Initialize results storage\n",
    "results = {\n",
    "    'trial': [],\n",
    "    'round': [],\n",
    "    'player': [],\n",
    "    'action': [],\n",
    "    'score': [],\n",
    "    'total_score': [],\n",
    "    'cooperation_rate': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments\n",
    "\n",
    "Execute multiple trials of the Prisoner's Dilemma experiment and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_trial(trial_num: int) -> Dict[str, Any]:\n",
    "    \"\"\"Run a single trial of the Prisoner's Dilemma experiment.\"\"\"\n",
    "    \n",
    "    # Initialize simulation with unique seed for each trial\n",
    "    seed = EXPERIMENT_CONFIG['random_seed'] + trial_num\n",
    "    simulation = Simulation(\n",
    "        gm_model=model,\n",
    "        agent_model=model,\n",
    "        embedder=embed_text,\n",
    "        measurements=measurements,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Run simulation\n",
    "    outcome, html_log = simulation()\n",
    "    \n",
    "    # Extract results\n",
    "    trial_results = {\n",
    "        'trial': trial_num,\n",
    "        'outcome': outcome,\n",
    "        'html_log': html_log\n",
    "    }\n",
    "    \n",
    "    return trial_results\n",
    "\n",
    "\n",
    "trial_results = []\n",
    "for trial in range(EXPERIMENT_CONFIG['num_trials']):\n",
    "    print(f\"Running trial {trial + 1}/{EXPERIMENT_CONFIG['num_trials']}\")\n",
    "    result = run_experiment_trial(trial)\n",
    "    trial_results.append(result)\n",
    "    \n",
    "    # Save HTML log for this trial\n",
    "    log_path = os.path.join(\n",
    "        EXPERIMENT_CONFIG['output_dir'],\n",
    "        f'trial_{trial + 1}_log.html'\n",
    "    )\n",
    "    with open(log_path, 'w') as f:\n",
    "        f.write(result['html_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results\n",
    "\n",
    "Process and visualize the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(trial_results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Convert trial results into a pandas DataFrame for analysis.\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for trial_num, result in enumerate(trial_results):\n",
    "        outcome = result['outcome']\n",
    "        \n",
    "        # Extract scores and metadata\n",
    "        for player_name, score in outcome.agent_scores.items():\n",
    "            data.append({\n",
    "                'trial': trial_num + 1,\n",
    "                'player': player_name,\n",
    "                'final_score': score,\n",
    "                'agent_type': next(\n",
    "                    agent_type for agent_type in EXPERIMENT_CONFIG['agent_types']\n",
    "                    if agent_type in player_name.lower()\n",
    "                )\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = analyze_results(trial_results)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(results_df.groupby('agent_type')['final_score'].agg(['mean', 'std', 'min', 'max']))\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\n",
    "    os.path.join(EXPERIMENT_CONFIG['output_dir'], 'experiment_results.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results\n",
    "\n",
    "Create visualizations to better understand the experiment outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations of the experiment results.\"\"\"\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Box plot of scores by agent type\n",
    "    sns.boxplot(data=df, x='agent_type', y='final_score', ax=ax1)\n",
    "    ax1.set_title('Score Distribution by Agent Type')\n",
    "    ax1.set_xlabel('Agent Type')\n",
    "    ax1.set_ylabel('Final Score')\n",
    "    \n",
    "    # Plot 2: Score trends across trials\n",
    "    for agent_type in df['agent_type'].unique():\n",
    "        agent_data = df[df['agent_type'] == agent_type]\n",
    "        ax2.plot(agent_data['trial'], agent_data['final_score'], \n",
    "                label=agent_type, marker='o')\n",
    "    \n",
    "    ax2.set_title('Score Trends Across Trials')\n",
    "    ax2.set_xlabel('Trial Number')\n",
    "    ax2.set_ylabel('Final Score')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EXPERIMENT_CONFIG['output_dir'], 'results_visualization.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Generate visualizations\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Experiment Report\n",
    "\n",
    "Create a comprehensive report of the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(df: pd.DataFrame):\n",
    "    \"\"\"Generate a comprehensive report of the experiment results.\"\"\"\n",
    "    \n",
    "    report = [\n",
    "        \"# Prisoner's Dilemma Experiment Report\\n\",\n",
    "        f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "        \"## Experiment Configuration\\n\",\n",
    "        f\"- Number of Trials: {EXPERIMENT_CONFIG['num_trials']}\",\n",
    "        f\"- Rounds per Trial: {EXPERIMENT_CONFIG['rounds_per_trial']}\",\n",
    "        f\"- Agent Types: {', '.join(EXPERIMENT_CONFIG['agent_types'])}\",\n",
    "        f\"- Random Seed: {EXPERIMENT_CONFIG['random_seed']}\\n\",\n",
    "        \"## Results Summary\\n\",\n",
    "        \"### Overall Statistics\\n\",\n",
    "        df.groupby('agent_type')['final_score'].agg(['mean', 'std', 'min', 'max']).to_markdown(),\n",
    "        \"\\n### Detailed Results\\n\",\n",
    "        df.to_markdown()\n",
    "    ]\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(EXPERIMENT_CONFIG['output_dir'], 'experiment_report.md')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# Generate and display report path\n",
    "report_path = generate_report(results_df)\n",
    "print(f\"Experiment report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
